{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GDL_PROJECT_V_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVnGFCpxV09_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b079a8-95c6-4283-acab-e17273ab7824"
      },
      "source": [
        "import torch\n",
        "try:\n",
        "  import torch_geometric\n",
        "except:\n",
        "  !pip -q install torch-scatter     -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-sparse      -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-cluster     -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-geometric\n",
        "  import torch_geometric\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.data import Data\n",
        "from scipy.sparse.csgraph import shortest_path\n",
        "from scipy.sparse import csr_matrix\n",
        "from torch_geometric.utils.random import erdos_renyi_graph\n",
        "import time\n",
        "import random\n",
        "from math import floor, ceil\n",
        "from copy import deepcopy\n",
        "\n",
        "from os import path\n",
        "import bz2\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6MB 7.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 7.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 389kB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 11.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 13.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXf2ZkFwr9Xs",
        "outputId": "388e4e70-236b-46f8-c9e2-2a88f4f35f7f"
      },
      "source": [
        "from google.colab import drive\n",
        "folder = '/content/drive/MyDrive/USI/GDLProject/'\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "sys.path.append(folder)\n",
        "\n",
        "from syntheticGraph import syntheticGraph\n",
        "from syntheticGraphDataset import syntheticGraphDataset\n",
        "from reducedGraph import reducedGraph\n",
        "from GINConv import GINConv\n",
        "from GNN import GNN\n",
        "\n",
        "torch.set_default_tensor_type(torch.FloatTensor)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSSV3yMGV0-N"
      },
      "source": [
        "loss_dict = dict()\n",
        "loss_avg = []\n",
        "test_loss = []\n",
        "losses = []\n",
        "\n",
        "hyperparams = {\n",
        "    'loss_epoch' : 0,\n",
        "    'lr' : 0.001,\n",
        "    'embedding_dim' : 50,\n",
        "    'n_layers' : 3,\n",
        "    'n_supernodes' : 100,\n",
        "    'n_eig' : 40,\n",
        "    'n_epochs' : 2,\n",
        "    'batch_size': 1,\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQAik7abtPAR",
        "outputId": "bcf4d008-d0bb-433f-eebd-4bd72d69e7f8"
      },
      "source": [
        "training_file = folder+'training_set.pbz2'\n",
        "valid_file = folder+'validation_set.pbz2'\n",
        "test_file = folder+'test_set.pbz2'\n",
        "training_set = syntheticGraphDataset.import_dataset(training_file)\n",
        "validation_set = syntheticGraphDataset.import_dataset(valid_file)\n",
        "test_set = syntheticGraphDataset.import_dataset(test_file)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the compressed set...\n",
            "Dataset loaded in  4.33670711517334 seconds\n",
            "\n",
            "Loading the compressed set...\n",
            "Dataset loaded in  7.7959253787994385 seconds\n",
            "\n",
            "Loading the compressed set...\n",
            "Dataset loaded in  53.11611270904541 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQjGPyvI7lZF"
      },
      "source": [
        "model = GNN(hyperparams['embedding_dim'], hyperparams['n_layers']).float()\n",
        "\n",
        "load = False\n",
        "if load:\n",
        "  model.load_state_dict(torch.load('state_dict_model.pt'))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = hyperparams['lr'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYLmpxowCzG2",
        "outputId": "42e43d80-fe3d-4a56-b179-28e29206bd27"
      },
      "source": [
        "with torch.enable_grad():\n",
        "  training_set.resetDispatcher()\n",
        "  training_set.reset_w_hat()\n",
        "\n",
        "  for graph in training_set.batchesIndices: \n",
        "    print(\"Graph \", graph)\n",
        "    losses = []\n",
        "    for epoch in range(hyperparams['n_epochs']):\n",
        "      print(\"\\tEpoch \", epoch, end=\" \")\n",
        "      start = time.time()\n",
        "      for batch in range(training_set.graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "        A, X, E, _, x, y = training_set.getNextBatch(graph, hyperparams['batch_size'])\n",
        "        out = model(A, X, E)\n",
        "        training_set.store_w_hat(graph, out, x, y)\n",
        "\n",
        "      loss = training_set.rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "      losses.append(loss.item())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      training_set.reset_w_hat(graph)\n",
        "      print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph  0\n",
            "\tEpoch  0  --- completed in  2.475904703140259 seconds with loss  1115213.25\n",
            "\tEpoch  1  --- completed in  2.372847318649292 seconds with loss  793335.9375\n",
            "Graph  1\n",
            "\tEpoch  0  --- completed in  2.466357946395874 seconds with loss  857614.875\n",
            "\tEpoch  1  --- completed in  2.475942850112915 seconds with loss  540596.125\n",
            "Graph  2\n",
            "\tEpoch  0  --- completed in  2.8549461364746094 seconds with loss  780649.75\n",
            "\tEpoch  1  --- completed in  2.842714786529541 seconds with loss  413602.09375\n",
            "Graph  3\n",
            "\tEpoch  0  --- completed in  2.966904878616333 seconds with loss  780929.6875\n",
            "\tEpoch  1  --- completed in  2.997213840484619 seconds with loss  949683.5625\n",
            "Graph  4\n",
            "\tEpoch  0  --- completed in  3.3611745834350586 seconds with loss  737627.75\n",
            "\tEpoch  1  --- completed in  3.436617612838745 seconds with loss  723390.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYZ_6OKsA_G9"
      },
      "source": [
        "# torch.save(model.state_dict(), \"state_dict_model2.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAlM24dFDhdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5f5f75-3e65-4bf9-cdb9-248759f6a762"
      },
      "source": [
        "with torch.no_grad():\n",
        "  improvements = []\n",
        "  test_set.reset_w_hat()\n",
        "  test_set.resetDispatcher()\n",
        "  for graph in test_set.batchesIndices: \n",
        "    print(\"Graph \", graph, end=\" \")\n",
        "    start = time.time()\n",
        "    loss = 0\n",
        "    for batch in range(test_set.graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "      A, X, E, _, x, y = test_set.getNextBatch(graph, hyperparams['batch_size'])\n",
        "      out = model(A, X, E)\n",
        "      test_set.store_w_hat(graph, out, x, y)\n",
        "    loss = test_set.rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "    imp = (test_set.originalGraphsLoss[graph] - loss)/test_set.originalGraphsLoss[graph]\n",
        "    improvements.append(imp)\n",
        "    print(\"--- relative improvement percentage \", imp.item()*100, \"%\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph  0 --- relative improvement percentage  29.79338765144348 %\n",
            "Graph  1 --- relative improvement percentage  24.912086129188538 %\n",
            "Graph  2 --- relative improvement percentage  17.965778708457947 %\n",
            "Graph  3 --- relative improvement percentage  19.178752601146698 %\n",
            "Graph  4 --- relative improvement percentage  18.964123725891113 %\n",
            "Graph  5 --- relative improvement percentage  16.427312791347504 %\n",
            "Graph  6 --- relative improvement percentage  13.370072841644287 %\n",
            "Graph  7 --- relative improvement percentage  14.781858026981354 %\n",
            "Graph  8 --- relative improvement percentage  12.223676592111588 %\n",
            "Graph  9 --- relative improvement percentage  11.243606358766556 %\n",
            "Graph  10 --- relative improvement percentage  10.709107667207718 %\n",
            "Graph  11 --- relative improvement percentage  9.578172117471695 %\n",
            "Graph  12 --- relative improvement percentage  9.585058689117432 %\n",
            "Graph  13 --- relative improvement percentage  9.248370677232742 %\n",
            "Graph  14 --- relative improvement percentage  9.758846461772919 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1buw0XFHMj5l"
      },
      "source": [
        "# OTHER ATTEMPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbCZeSRr9U-l",
        "outputId": "e5a71ee7-9c21-4b41-8067-9fab7bd5dfa6"
      },
      "source": [
        "model = GNN(hyperparams['embedding_dim'], hyperparams['n_layers']).float()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = hyperparams['lr'])\n",
        "\n",
        "with torch.enable_grad():\n",
        "  training_set.resetDispatcher()\n",
        "  training_set.reset_w_hat()\n",
        "  for epoch in range(hyperparams['n_epochs']):\n",
        "    print(\"Epoch \", epoch)\n",
        "    losses = []\n",
        "    for graph in training_set.batchesIndices: \n",
        "      print(\"\\tGraph \", graph, end=\" \")\n",
        "      start = time.time()\n",
        "      for batch in range(training_set.graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "        A, X, E, _, x, y = training_set.getNextBatch(graph, hyperparams['batch_size'])\n",
        "        out = model(A, X, E)\n",
        "        training_set.store_w_hat(graph, out, x, y)\n",
        "\n",
        "      loss = training_set.rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "      losses.append(loss.item())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      training_set.reset_w_hat(graph)\n",
        "      print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())\n",
        "    training_set.shuffle()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0\n",
            "\tGraph  0  --- completed in  2.351430892944336 seconds with loss  187934.640625\n",
            "\tGraph  1  --- completed in  2.4727249145507812 seconds with loss  1060777.25\n",
            "\tGraph  2  --- completed in  2.7637321949005127 seconds with loss  1333261.0\n",
            "\tGraph  3  --- completed in  3.0248165130615234 seconds with loss  965242.0625\n",
            "\tGraph  4  --- completed in  3.376180648803711 seconds with loss  1424756.375\n",
            "Epoch  1\n",
            "\tGraph  3  --- completed in  2.961644172668457 seconds with loss  509789.15625\n",
            "\tGraph  2  --- completed in  2.876694679260254 seconds with loss  274335.125\n",
            "\tGraph  0  --- completed in  2.3509979248046875 seconds with loss  1571025.0\n",
            "\tGraph  4  --- completed in  3.3066422939300537 seconds with loss  813699.625\n",
            "\tGraph  1  --- completed in  2.5115864276885986 seconds with loss  938868.625\n",
            "Epoch  2\n",
            "\tGraph  1  --- completed in  2.498171806335449 seconds with loss  815024.1875\n",
            "\tGraph  2  --- completed in  2.826765298843384 seconds with loss  320499.5\n",
            "\tGraph  4  --- completed in  3.4414737224578857 seconds with loss  1127613.25\n",
            "\tGraph  3  --- completed in  2.9632105827331543 seconds with loss  405857.875\n",
            "\tGraph  0  --- completed in  2.425130605697632 seconds with loss  798846.5\n",
            "Epoch  3\n",
            "\tGraph  4  --- completed in  3.31982684135437 seconds with loss  1310866.75\n",
            "\tGraph  1  --- completed in  2.537834405899048 seconds with loss  284267.53125\n",
            "\tGraph  0  --- completed in  2.346414804458618 seconds with loss  443139.34375\n",
            "\tGraph  3  --- completed in  2.9261529445648193 seconds with loss  651140.625\n",
            "\tGraph  2  --- completed in  2.857851266860962 seconds with loss  882782.125\n",
            "Epoch  4\n",
            "\tGraph  3  --- completed in  2.9238696098327637 seconds with loss  707718.0625\n",
            "\tGraph  4  --- completed in  3.3048033714294434 seconds with loss  1541856.125\n",
            "\tGraph  1  --- completed in  2.48358416557312 seconds with loss  222257.8125\n",
            "\tGraph  0  --- completed in  2.360283374786377 seconds with loss  466850.9375\n",
            "\tGraph  2  --- completed in  2.8415350914001465 seconds with loss  656786.5\n",
            "Epoch  5\n",
            "\tGraph  4  --- completed in  3.3411123752593994 seconds with loss  1368872.375\n",
            "\tGraph  1  --- completed in  2.534562587738037 seconds with loss  322391.75\n",
            "\tGraph  0  --- completed in  2.3440401554107666 seconds with loss  673140.625\n",
            "\tGraph  2  --- completed in  2.7832725048065186 seconds with loss  529536.875\n",
            "\tGraph  3  --- completed in  2.915524959564209 seconds with loss  397998.4375\n",
            "Epoch  6\n",
            "\tGraph  4  --- completed in  3.340174913406372 seconds with loss  1249525.375\n",
            "\tGraph  2  --- completed in  3.230053424835205 seconds with loss  433786.59375\n",
            "\tGraph  1  --- completed in  2.868081569671631 seconds with loss  517947.4375\n",
            "\tGraph  0  --- completed in  2.756920099258423 seconds with loss  982169.25\n",
            "\tGraph  3  --- completed in  3.4024300575256348 seconds with loss  426092.125\n",
            "Epoch  7\n",
            "\tGraph  4  --- completed in  3.657010555267334 seconds with loss  1152821.125\n",
            "\tGraph  0  --- completed in  2.3389244079589844 seconds with loss  824757.4375\n",
            "\tGraph  2  --- completed in  2.774839401245117 seconds with loss  469240.625\n",
            "\tGraph  3  --- completed in  2.9156548976898193 seconds with loss  393866.125\n",
            "\tGraph  1  --- completed in  2.588386297225952 seconds with loss  347538.625\n",
            "Epoch  8\n",
            "\tGraph  0  --- completed in  2.3463118076324463 seconds with loss  602906.75\n",
            "\tGraph  2  --- completed in  2.7967355251312256 seconds with loss  642869.9375\n",
            "\tGraph  1  --- completed in  2.482945203781128 seconds with loss  234271.546875\n",
            "\tGraph  3  --- completed in  2.9736034870147705 seconds with loss  559977.6875\n",
            "\tGraph  4  --- completed in  3.258692502975464 seconds with loss  1491499.75\n",
            "Epoch  9\n",
            "\tGraph  1  --- completed in  2.4888741970062256 seconds with loss  217430.1875\n",
            "\tGraph  0  --- completed in  2.4373817443847656 seconds with loss  379645.34375\n",
            "\tGraph  3  --- completed in  3.0226268768310547 seconds with loss  570673.375\n",
            "\tGraph  4  --- completed in  3.3013479709625244 seconds with loss  1479429.0\n",
            "\tGraph  2  --- completed in  2.8159632682800293 seconds with loss  703628.8125\n",
            "Epoch  10\n",
            "\tGraph  0  --- completed in  2.3729097843170166 seconds with loss  493454.25\n",
            "\tGraph  4  --- completed in  3.287783622741699 seconds with loss  1357941.5\n",
            "\tGraph  3  --- completed in  3.063767433166504 seconds with loss  411341.71875\n",
            "\tGraph  2  --- completed in  2.8350720405578613 seconds with loss  491495.84375\n",
            "\tGraph  1  --- completed in  2.512763500213623 seconds with loss  457524.875\n",
            "Epoch  11\n",
            "\tGraph  4  --- completed in  3.2716925144195557 seconds with loss  1098920.875\n",
            "\tGraph  1  --- completed in  2.5104877948760986 seconds with loss  610107.5\n",
            "\tGraph  0  --- completed in  2.3578717708587646 seconds with loss  1078188.875\n",
            "\tGraph  3  --- completed in  2.9959945678710938 seconds with loss  461890.9375\n",
            "\tGraph  2  --- completed in  2.8559820652008057 seconds with loss  373246.0\n",
            "Epoch  12\n",
            "\tGraph  0  --- completed in  2.366302490234375 seconds with loss  847047.1875\n",
            "\tGraph  3  --- completed in  2.983248472213745 seconds with loss  392114.0625\n",
            "\tGraph  2  --- completed in  2.8298513889312744 seconds with loss  544930.0\n",
            "\tGraph  4  --- completed in  3.307694673538208 seconds with loss  1329158.25\n",
            "\tGraph  1  --- completed in  2.4739198684692383 seconds with loss  300288.84375\n",
            "Epoch  13\n",
            "\tGraph  3  --- completed in  2.8985965251922607 seconds with loss  427595.8125\n",
            "\tGraph  2  --- completed in  2.7941975593566895 seconds with loss  594498.625\n",
            "\tGraph  4  --- completed in  3.397179126739502 seconds with loss  1315465.0\n",
            "\tGraph  1  --- completed in  2.4930005073547363 seconds with loss  357006.21875\n",
            "\tGraph  0  --- completed in  2.371026039123535 seconds with loss  720000.1875\n",
            "Epoch  14\n",
            "\tGraph  0  --- completed in  2.467263698577881 seconds with loss  677107.5\n",
            "\tGraph  4  --- completed in  3.4061505794525146 seconds with loss  1330017.125\n",
            "\tGraph  2  --- completed in  2.937720537185669 seconds with loss  613223.0\n",
            "\tGraph  1  --- completed in  2.494819402694702 seconds with loss  276261.125\n",
            "\tGraph  3  --- completed in  2.9549622535705566 seconds with loss  445695.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZJgnr9sMIY_"
      },
      "source": [
        "# #TRAINING SET\n",
        "# start = time.time()\n",
        "# folder = '/content/drive/MyDrive/USI/GDLProject/'\n",
        "# training_name = folder+'training_set.pbz2'\n",
        "# valid_name = folder+'validation_set.pbz2'\n",
        "# test_name = folder+'test_set.pbz2'\n",
        "\n",
        "# train = path.isfile(training_name)\n",
        "# valid = path.isfile(valid_name)\n",
        "# test = path.isfile(test_name)\n",
        "# compressed = True\n",
        "\n",
        "# if (train):\n",
        "#   print(\"Loading the compressed training set...\")\n",
        "#   training_set = decompress_pickle(training_name) \n",
        "#   print(\"Compressed training set loaded in \", time.time()-start, \"seconds\\n\")\n",
        "# else:\n",
        "#   print(\"Creating the training set...\")\n",
        "#   training_set = syntheticGraphDataset(entireMatrix=False)\n",
        "#   sizes = [i for i in range(512, 1012, 100)]\n",
        "#   preprocess = [True] * len(sizes)\n",
        "#   for key, s in enumerate(sizes):\n",
        "#     print(\"\\t Creating graph \", key, \" of size \", s)\n",
        "#     graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s,  p = (0.1*512)/s)\n",
        "#     red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes=hyperparams['n_supernodes'])\n",
        "#     training_set.addGraph(graph, red_graph, hyperparams['n_eig'], preprocessData = preprocess[key])\n",
        "#   print(\"Training set created in \", time.time()-start, \"seconds\\n\")\n",
        "\n",
        "# #VALIDATION SET\n",
        "# start = time.time()\n",
        "# if (valid):\n",
        "#   print(\"Loading the compressed validation set...\")\n",
        "#   validation_set = decompress_pickle(valid_name)\n",
        "#   print(\"Compressed validation set loaded in \", time.time()-start, \"seconds\\n\")\n",
        "# else:\n",
        "#   print(\"Creating the validation set...\")\n",
        "#   validation_set = syntheticGraphDataset(entireMatrix=False)\n",
        "#   sizes = [i for i in range(1012, 1512, 100)]\n",
        "#   preprocess = [True]*len(sizes)\n",
        "#   for key, s in enumerate(sizes):\n",
        "#     print(\"\\tCreating graph \", key, \" of size \", s)\n",
        "#     graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s,  p = (0.1*512)/s)\n",
        "#     red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes=hyperparams['n_supernodes'])\n",
        "#     validation_set.addGraph(graph, red_graph, hyperparams['n_eig'], preprocessData = preprocess[key])\n",
        "#   print(\"Validation set created in \", time.time()-start, \"seconds\\n\")\n",
        "\n",
        "\n",
        "# #TEST SET\n",
        "# start = time.time()\n",
        "# if (test):\n",
        "#   print(\"Loading the compressed test set...\")\n",
        "#   test_set = decompress_pickle(test_name)\n",
        "#   print(\"Compressed test set loaded in \", time.time()-start, \"seconds\\n\")\n",
        "# else:\n",
        "#   print(\"Creating the test set...\")\n",
        "#   test_set = syntheticGraphDataset(entireMatrix=False)\n",
        "#   sizes = [i for i in range(1512, 2913, 100)]\n",
        "#   preprocess = [True]*len(sizes)\n",
        "#   for key, s in enumerate(sizes):\n",
        "#     print(\"\\tCreating graph \", key, \" of size \",s)\n",
        "#     graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s, p = (0.1*512)/s)\n",
        "#     red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes = hyperparams['n_supernodes'])\n",
        "#     test_set.addGraph(graph, red_graph, hyperparams['n_eig'] , preprocessData = preprocess[key])\n",
        "#   print(\"Dataset created in \", time.time() - start, \"seconds\")\n",
        "\n",
        "\n",
        "# if not train:\n",
        "#   compressed_pickle(folder+training_name, training_set)\n",
        "# if not valid:\n",
        "#   compressed_pickle(folder+valid_name, validation_set)\n",
        "# if not test:\n",
        "#   compressed_pickle(folder+test_name, test_set)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRaVBQAR_ujz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}