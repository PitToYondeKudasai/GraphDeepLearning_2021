{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GDL_PROJECT_V_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVnGFCpxV09_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e7007c-604a-4447-85a5-d17377e99dbf"
      },
      "source": [
        "import torch\n",
        "try:\n",
        "  import torch_geometric\n",
        "except:\n",
        "  !pip -q install torch-scatter     -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-sparse      -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-cluster     -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-geometric\n",
        "  import torch_geometric\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.data import Data\n",
        "from scipy.sparse.csgraph import shortest_path\n",
        "from scipy.sparse import csr_matrix\n",
        "from torch_geometric.utils.random import erdos_renyi_graph\n",
        "import time\n",
        "import random\n",
        "from math import floor, ceil\n",
        "from copy import deepcopy\n",
        "import importlib\n",
        "from os import path, makedirs\n",
        "import bz2\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "import sys\n",
        "from google.colab import drive\n",
        "folder = '/content/drive/MyDrive/USI/GDLProject/'\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        " \n",
        "sys.path.append(folder)\n",
        " \n",
        "\n",
        "import syntheticGraph\n",
        "import syntheticGraphDataset\n",
        "import reducedGraph\n",
        "import GINConv\n",
        "import GNN\n",
        "import graph\n",
        "\n",
        "importlib.reload(syntheticGraph)\n",
        "importlib.reload(syntheticGraphDataset)\n",
        "importlib.reload(reducedGraph)\n",
        "importlib.reload(GINConv)\n",
        "importlib.reload(GNN)\n",
        "importlib.reload(graph)\n",
        "\n",
        "from graph import Graph, Node\n",
        "from syntheticGraph import syntheticGraph\n",
        "from syntheticGraphDataset import syntheticGraphDataset\n",
        "from reducedGraph import reducedGraph\n",
        "from GINConv import GINConv\n",
        "from GNN import GNN\n",
        " \n",
        "torch.set_default_tensor_type(torch.FloatTensor)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH7d8vt8YoEH"
      },
      "source": [
        "def exportData(trainings, validations, tests, folder, rr):\n",
        "\n",
        "  a = np.asarray(trainings)\n",
        "  np.savetxt(folder + \"training_losses_\"+rr+\".csv\", a, delimiter=\";\")\n",
        "  b = np.asarray(validations)\n",
        "  np.savetxt(folder + \"validation_losses_\"+rr+\".csv\", b, delimiter=\";\")\n",
        "  c = np.asarray(tests)\n",
        "  np.savetxt(folder + \"test_losses_\"+rr+\".csv\", c, delimiter=\";\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSb4AfCGILHY"
      },
      "source": [
        "def train(model, datasets, hyperparams, verbose=False, exportModel=False, exportEvaluationData = False, folder = ''):\n",
        "  validations = []\n",
        "  tests = []\n",
        "  trainings = []\n",
        "\n",
        "  if (verbose): initial_start = time.time()\n",
        "  with torch.enable_grad():\n",
        "    datasets[\"training_set\"].resetDispatcher()\n",
        "    datasets[\"training_set\"].reset_w_hat()\n",
        "  \n",
        "    for graph in datasets[\"training_set\"].batchesIndices: \n",
        "      if (verbose): print(\"GRAPH \", graph)\n",
        "      losses = []\n",
        "      for epoch in range(hyperparams['n_epochs']):\n",
        "        if (verbose): print(\"\\tEpoch \", epoch, end=\" \")\n",
        "        start = time.time()\n",
        "        for batch in range(datasets[\"training_set\"].graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "          A, X, E, _, x, y = datasets[\"training_set\"].getNextBatch(graph, hyperparams['batch_size'])\n",
        "          out = model(A, X, E)\n",
        "          datasets[\"training_set\"].store_w_hat(graph, out, x, y)\n",
        "  \n",
        "        loss = datasets[\"training_set\"].rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "        losses.append(loss.item())\n",
        "        model.backpropagate(loss)\n",
        "        datasets[\"training_set\"].reset_w_hat(graph)\n",
        "        if (verbose): print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())\n",
        "      trainings.append(losses)\n",
        "      validations.append(model.evaluateRayleighLoss(datasets[\"validation_set\"], hyperparams, verbose = verbose))\n",
        "      print(\"validations\", validations)\n",
        "  if (verbose): print(\"Total training completed in \", time.time()-initial_start, \"seconds\")\n",
        "  if (exportModel): model.export(folder+\"model_\"+str(hyperparams['reduction_ratio'])+\".pt\")\n",
        "  tests.append(model.evaluateRayleighLoss(datasets[\"test_set\"], hyperparams, verbose = verbose))\n",
        "  if (exportEvaluationData): exportData(trainings, validations, tests, folder, rr = str(hyperparams['reduction_ratio']))\n",
        "  return trainings, validations, tests"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rDyWPbEQI6q"
      },
      "source": [
        "folder = '/content/drive/MyDrive/USI/GDLProject/'\n",
        "name_list = [\"training_set\", \"validation_set\", \"test_set\"]\n",
        "# reduction_ratio_list = [0.3, 0.5, 0.7]\n",
        "name_graph_class_list = ['barabasi_albert_graph' ,'erdos_renyi_graph']\n",
        "reducution_type_list = ['hem','baseline']\n",
        "# reduction_ratio_list = [0.3]\n",
        "# name_graph_class_list = ['erdos_renyi_graph']\n",
        "# reducution_type_list = ['baseline']\n",
        "\n",
        "\n",
        "hyperparams = {\n",
        "    'loss_epoch' : 0,\n",
        "    'lr' : 0.001,\n",
        "    'embedding_dim' : 50,\n",
        "    'n_layers' : 3,\n",
        "    'n_eig' : 40,\n",
        "    'n_epochs' : 1,\n",
        "    'batch_size': 1,\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4eujxduHGKe",
        "outputId": "b463805f-a672-4e17-8aeb-a13a1551435d"
      },
      "source": [
        "datasets = {}\n",
        "\n",
        "for reduction_ratio in reduction_ratio_list:\n",
        "  hyperparams['reduction_ratio'] = reduction_ratio\n",
        "  for name_graph_class in name_graph_class_list:\n",
        "    for reducution_type in reducution_type_list:\n",
        "      \n",
        "      subfolder = 'trained_models/'+name_graph_class+'/'+reducution_type+'/'\n",
        "      if not path.exists(folder+subfolder):\n",
        "          os.makedirs(folder+subfolder)\n",
        "\n",
        "      for name in name_list:\n",
        "        dataset_name = folder+name+'_'+str(reduction_ratio)+\"_\"+name_graph_class+'_'+reducution_type+'_'+'.pbz2'\n",
        "        if(name == \"training_set\"):\n",
        "          datasets[name] = syntheticGraphDataset.import_dataset(dataset_name, verbose = True)\n",
        "        else:\n",
        "          datasets[name]=datasets[\"training_set\"]\n",
        "      model = GNN(hyperparams['embedding_dim'], hyperparams['n_layers'], hyperparams['lr'], pathname = None)\n",
        "      trainings, validations, tests = train(model, datasets, hyperparams, verbose=True, exportModel=True, exportEvaluationData = True, folder=folder+subfolder)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the compressed set...\n",
            "Dataset loaded in  9.761805772781372 seconds\n",
            "\n",
            "GRAPH  0\n",
            "\tEpoch  0  --- completed in  5.765455722808838 seconds with loss  14.685006141662598\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -301.52368545532227 %\n",
            "Graph  1 --- relative improvement percentage  -153.39410305023193 %\n",
            "Graph  2 --- relative improvement percentage  -235.7372522354126 %\n",
            "Graph  3 --- relative improvement percentage  -215.32669067382812 %\n",
            "Graph  4 --- relative improvement percentage  -164.81797695159912 %\n",
            "validations [[-3.0152368545532227, -1.5339410305023193, -2.357372522354126, -2.1532669067382812, -1.6481797695159912]]\n",
            "GRAPH  1\n",
            "\tEpoch  0  --- completed in  8.209980487823486 seconds with loss  20.42694854736328\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  60.592347383499146 %\n",
            "Graph  1 --- relative improvement percentage  54.67997193336487 %\n",
            "Graph  2 --- relative improvement percentage  55.18280863761902 %\n",
            "Graph  3 --- relative improvement percentage  42.00925529003143 %\n",
            "Graph  4 --- relative improvement percentage  41.50480329990387 %\n",
            "validations [[-3.0152368545532227, -1.5339410305023193, -2.357372522354126, -2.1532669067382812, -1.6481797695159912], [0.6059234738349915, 0.5467997193336487, 0.5518280863761902, 0.42009255290031433, 0.4150480329990387]]\n",
            "GRAPH  2\n",
            "\tEpoch  0  --- completed in  9.684059858322144 seconds with loss  3.0981855392456055\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -90.77500700950623 %\n",
            "Graph  1 --- relative improvement percentage  -27.37797200679779 %\n",
            "Graph  2 --- relative improvement percentage  -37.51629889011383 %\n",
            "Graph  3 --- relative improvement percentage  -0.813498068600893 %\n",
            "Graph  4 --- relative improvement percentage  6.011418253183365 %\n",
            "validations [[-3.0152368545532227, -1.5339410305023193, -2.357372522354126, -2.1532669067382812, -1.6481797695159912], [0.6059234738349915, 0.5467997193336487, 0.5518280863761902, 0.42009255290031433, 0.4150480329990387], [-0.9077500700950623, -0.2737797200679779, -0.3751629889011383, -0.00813498068600893, 0.06011418253183365]]\n",
            "GRAPH  3\n",
            "\tEpoch  0  --- completed in  12.37730860710144 seconds with loss  7.512900352478027\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -148.33776950836182 %\n",
            "Graph  1 --- relative improvement percentage  -63.90485167503357 %\n",
            "Graph  2 --- relative improvement percentage  -77.58198976516724 %\n",
            "Graph  3 --- relative improvement percentage  -45.93171775341034 %\n",
            "Graph  4 --- relative improvement percentage  -30.60784637928009 %\n",
            "validations [[-3.0152368545532227, -1.5339410305023193, -2.357372522354126, -2.1532669067382812, -1.6481797695159912], [0.6059234738349915, 0.5467997193336487, 0.5518280863761902, 0.42009255290031433, 0.4150480329990387], [-0.9077500700950623, -0.2737797200679779, -0.3751629889011383, -0.00813498068600893, 0.06011418253183365], [-1.4833776950836182, -0.6390485167503357, -0.7758198976516724, -0.4593171775341034, -0.3060784637928009]]\n",
            "GRAPH  4\n",
            "\tEpoch  0  --- completed in  15.63970422744751 seconds with loss  10.738319396972656\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -161.8128776550293 %\n",
            "Graph  1 --- relative improvement percentage  -72.00685739517212 %\n",
            "Graph  2 --- relative improvement percentage  -86.94078326225281 %\n",
            "Graph  3 --- relative improvement percentage  -55.8720588684082 %\n",
            "Graph  4 --- relative improvement percentage  -38.90747129917145 %\n",
            "validations [[-3.0152368545532227, -1.5339410305023193, -2.357372522354126, -2.1532669067382812, -1.6481797695159912], [0.6059234738349915, 0.5467997193336487, 0.5518280863761902, 0.42009255290031433, 0.4150480329990387], [-0.9077500700950623, -0.2737797200679779, -0.3751629889011383, -0.00813498068600893, 0.06011418253183365], [-1.4833776950836182, -0.6390485167503357, -0.7758198976516724, -0.4593171775341034, -0.3060784637928009], [-1.618128776550293, -0.7200685739517212, -0.8694078326225281, -0.558720588684082, -0.3890747129917145]]\n",
            "Total training completed in  146.05101108551025 seconds\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -161.8128776550293 %\n",
            "Graph  1 --- relative improvement percentage  -72.00685739517212 %\n",
            "Graph  2 --- relative improvement percentage  -86.94078326225281 %\n",
            "Graph  3 --- relative improvement percentage  -55.8720588684082 %\n",
            "Graph  4 --- relative improvement percentage  -38.90747129917145 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HpV9MamHFwP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTvXUG56cXpH"
      },
      "source": [
        "# initial_start = time.time()\n",
        "\n",
        "# with torch.enable_grad():\n",
        "#   training_set.resetDispatcher()\n",
        "#   training_set.reset_w_hat()\n",
        "#   for e in range(3):\n",
        "#     print(\"Big epoch \", e)\n",
        "#     for graph in training_set.batchesIndices: \n",
        "#       print(\"\\tGraph \", graph)\n",
        "#       losses = []\n",
        "#       for epoch in range(hyperparams['n_epochs'], ):\n",
        "#         print(\"\\t\\tEpoch \", epoch, end=\" \")\n",
        "#         start = time.time()\n",
        "#         for batch in range(training_set.graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "#           A, X, E, _, x, y = training_set.getNextBatch(graph, hyperparams['batch_size'])\n",
        "#           out = model(A, X, E)\n",
        "#           training_set.store_w_hat(graph, out, x, y)\n",
        "\n",
        "#         loss = training_set.rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "#         losses.append(loss.item())\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         optimizer.zero_grad()\n",
        "#         training_set.reset_w_hat(graph)\n",
        "#         print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())\n",
        "#     training_set.shuffle()\n",
        "# print(\"Total training completed in \", time.time()-initial_start, \"seconds\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbCZeSRr9U-l"
      },
      "source": [
        "# model = GNN(hyperparams['embedding_dim'], hyperparams['n_layers']).float()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = hyperparams['lr'])\n",
        "\n",
        "# with torch.enable_grad():\n",
        "#   training_set.resetDispatcher()\n",
        "#   training_set.reset_w_hat()\n",
        "#   for epoch in range(hyperparams['n_epochs']):\n",
        "#     print(\"Epoch \", epoch)\n",
        "#     losses = []\n",
        "#     for graph in training_set.batchesIndices: \n",
        "#       print(\"\\tGraph \", graph, end=\" \")\n",
        "#       start = time.time()\n",
        "#       for batch in range(training_set.graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "#         A, X, E, _, x, y = training_set.getNextBatch(graph, hyperparams['batch_size'])\n",
        "#         out = model(A, X, E)\n",
        "#         training_set.store_w_hat(graph, out, x, y)\n",
        "\n",
        "#       loss = training_set.rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "#       losses.append(loss.item())\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#       optimizer.zero_grad()\n",
        "#       training_set.reset_w_hat(graph)\n",
        "#       print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())\n",
        "#     training_set.shuffle()"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}