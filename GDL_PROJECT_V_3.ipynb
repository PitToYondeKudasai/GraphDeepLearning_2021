{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GDL_PROJECT_V_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVnGFCpxV09_",
        "outputId": "7c1a6863-e6aa-4bfd-f33b-ae17dbeafb7a"
      },
      "source": [
        "import torch\n",
        "try:\n",
        "  import torch_geometric\n",
        "except:\n",
        "  !pip -q install torch-scatter     -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-sparse      -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-cluster     -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-geometric\n",
        "  import torch_geometric\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.data import Data\n",
        "from scipy.sparse.csgraph import shortest_path\n",
        "from scipy.sparse import csr_matrix\n",
        "from torch_geometric.utils.random import erdos_renyi_graph\n",
        "import time\n",
        "import random\n",
        "from math import floor, ceil\n",
        "from copy import deepcopy\n",
        "\n",
        "from os import path\n",
        "import bz2\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6MB 27.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 30.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 389kB 19.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 27.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 43.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 45.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXf2ZkFwr9Xs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f6eb5b1-8de9-4033-b7f6-23436c88072c"
      },
      "source": [
        "from google.colab import drive\n",
        "folder = '/content/drive/MyDrive/USI/GDLProject/'\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "sys.path.append(folder)\n",
        "\n",
        "from syntheticGraph import syntheticGraph\n",
        "from syntheticGraphDataset import syntheticGraphDataset\n",
        "from reducedGraph import reducedGraph\n",
        "from GINConv import GINConv\n",
        "from GNN import GNN\n",
        "\n",
        "torch.set_default_tensor_type(torch.FloatTensor)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSSV3yMGV0-N"
      },
      "source": [
        "loss_dict = dict()\n",
        "loss_avg = []\n",
        "test_loss = []\n",
        "losses = []\n",
        "\n",
        "hyperparams = {\n",
        "    'loss_epoch' : 0,\n",
        "    'lr' : 0.001,\n",
        "    'embedding_dim' : 50,\n",
        "    'n_layers' : 3,\n",
        "    'n_supernodes' : 100,\n",
        "    'n_eig' : 40,\n",
        "    'n_epochs' : 5,\n",
        "    'batch_size': 1,\n",
        "    'reduction_ratio':0.5,\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQAik7abtPAR",
        "outputId": "ff701979-f4ed-4b23-a4b4-69d5611d49f9"
      },
      "source": [
        "training_file = folder+'training_set_'+str(hyperparams['reduction_ratio'])+'.pbz2'\n",
        "valid_file = folder+'validation_set_'+str(hyperparams['reduction_ratio'])+'.pbz2'\n",
        "test_file = folder+'test_set_'+str(hyperparams['reduction_ratio'])+'.pbz2'\n",
        "training_set = syntheticGraphDataset.import_dataset(training_file)\n",
        "validation_set = syntheticGraphDataset.import_dataset(valid_file)\n",
        "# test_set = syntheticGraphDataset.import_dataset(test_file)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the compressed set...\n",
            "Dataset loaded in  9.94755220413208 seconds\n",
            "\n",
            "Loading the compressed set...\n",
            "Dataset loaded in  20.49639081954956 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQjGPyvI7lZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd75fc98-8c07-4bb4-f7c8-c443acc0dea4"
      },
      "source": [
        "model = GNN(hyperparams['embedding_dim'], hyperparams['n_layers']).float()\n",
        "\n",
        "load = False\n",
        "if load:\n",
        "  model.load_state_dict(torch.load('state_dict_model.pt'))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = hyperparams['lr'])\n",
        "\n",
        "_ = model.evaluate(validation_set, hyperparams, verbose = True)\n",
        "# print()\n",
        "# _ = model.evaluate(test_set, hyperparams, verbose = True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -202700.72021484375 %\n",
            "Graph  1 --- relative improvement percentage  -157357.45849609375 %\n",
            "Graph  2 --- relative improvement percentage  -184975.37841796875 %\n",
            "Graph  3 --- relative improvement percentage  -194662.48779296875 %\n",
            "Graph  4 --- relative improvement percentage  -152568.44482421875 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAsSNGJ88MHR"
      },
      "source": [
        "# _ = model.evaluate(training_set, hyperparams, verbose = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYLmpxowCzG2",
        "outputId": "7f2ad782-d72d-4443-a209-00de57b48654"
      },
      "source": [
        "initial_start = time.time()\n",
        "with torch.enable_grad():\n",
        "  training_set.resetDispatcher()\n",
        "  training_set.reset_w_hat()\n",
        "\n",
        "  for graph in training_set.batchesIndices: \n",
        "    print(\"Graph \", graph)\n",
        "    losses = []\n",
        "    for epoch in range(hyperparams['n_epochs']):\n",
        "      print(\"\\tEpoch \", epoch, end=\" \")\n",
        "      start = time.time()\n",
        "      for batch in range(training_set.graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "        A, X, E, _, x, y = training_set.getNextBatch(graph, hyperparams['batch_size'])\n",
        "        out = model(A, X, E)\n",
        "        training_set.store_w_hat(graph, out, x, y)\n",
        "\n",
        "      loss = training_set.rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "      losses.append(loss.item())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      training_set.reset_w_hat(graph)\n",
        "      print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())\n",
        "    print()\n",
        "    _ = model.evaluate(validation_set, hyperparams, verbose = True)\n",
        "    print()\n",
        "print(\"Total training completed in \", time.time()-initial_start, \"seconds\")\n",
        "print()\n",
        "# _ = model.evaluate(test_set, hyperparams, verbose = True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph  0\n",
            "\tEpoch  0  --- completed in  7.053288698196411 seconds with loss  11167.8896484375\n",
            "\tEpoch  1  --- completed in  7.379492521286011 seconds with loss  3919.178466796875\n",
            "\tEpoch  2  --- completed in  7.416180372238159 seconds with loss  43.251895904541016\n",
            "\tEpoch  3  --- completed in  7.493308067321777 seconds with loss  73.9354019165039\n",
            "\tEpoch  4  --- completed in  7.3314526081085205 seconds with loss  89.17642211914062\n",
            "\n",
            "Evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -1582.5979232788086 %\n",
            "Graph  1 --- relative improvement percentage  -1239.4184112548828 %\n",
            "Graph  2 --- relative improvement percentage  -1424.1143226623535 %\n",
            "Graph  3 --- relative improvement percentage  -1479.2683601379395 %\n",
            "Graph  4 --- relative improvement percentage  -1161.3075256347656 %\n",
            "\n",
            "Graph  1\n",
            "\tEpoch  0  --- completed in  10.100066184997559 seconds with loss  102.94280242919922\n",
            "\tEpoch  1  --- completed in  10.114293336868286 seconds with loss  109.97029876708984\n",
            "\tEpoch  2  --- completed in  10.314839363098145 seconds with loss  114.81562042236328\n",
            "\tEpoch  3  --- completed in  9.994123935699463 seconds with loss  117.50762939453125\n",
            "\tEpoch  4  --- completed in  10.039421558380127 seconds with loss  117.7698745727539\n",
            "\n",
            "Evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -1818.947410583496 %\n",
            "Graph  1 --- relative improvement percentage  -1434.530258178711 %\n",
            "Graph  2 --- relative improvement percentage  -1650.7152557373047 %\n",
            "Graph  3 --- relative improvement percentage  -1708.8172912597656 %\n",
            "Graph  4 --- relative improvement percentage  -1347.5312232971191 %\n",
            "\n",
            "Graph  2\n",
            "\tEpoch  0  --- completed in  12.853123903274536 seconds with loss  115.03839111328125\n",
            "\tEpoch  1  --- completed in  12.724558115005493 seconds with loss  112.73217010498047\n",
            "\tEpoch  2  --- completed in  12.814103603363037 seconds with loss  109.18855285644531\n",
            "\tEpoch  3  --- completed in  12.786475896835327 seconds with loss  104.61581420898438\n",
            "\tEpoch  4  --- completed in  12.873749732971191 seconds with loss  99.64327239990234\n",
            "\n",
            "Evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -1487.417984008789 %\n",
            "Graph  1 --- relative improvement percentage  -1180.6500434875488 %\n",
            "Graph  2 --- relative improvement percentage  -1356.7587852478027 %\n",
            "Graph  3 --- relative improvement percentage  -1393.0803298950195 %\n",
            "Graph  4 --- relative improvement percentage  -1102.455997467041 %\n",
            "\n",
            "Graph  3\n",
            "\tEpoch  0  --- completed in  16.48098611831665 seconds with loss  92.99964141845703\n",
            "\tEpoch  1  --- completed in  16.47679615020752 seconds with loss  89.2611312866211\n",
            "\tEpoch  2  --- completed in  16.390034914016724 seconds with loss  85.40410614013672\n",
            "\tEpoch  3  --- completed in  16.617544174194336 seconds with loss  81.24136352539062\n",
            "\tEpoch  4  --- completed in  16.116806030273438 seconds with loss  76.52519226074219\n",
            "\n",
            "Evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -1092.9264068603516 %\n",
            "Graph  1 --- relative improvement percentage  -871.826171875 %\n",
            "Graph  2 --- relative improvement percentage  -999.5087623596191 %\n",
            "Graph  3 --- relative improvement percentage  -1016.6125297546387 %\n",
            "Graph  4 --- relative improvement percentage  -804.4713973999023 %\n",
            "\n",
            "Graph  4\n",
            "\tEpoch  0  --- completed in  19.554004907608032 seconds with loss  77.29708099365234\n",
            "\tEpoch  1  --- completed in  19.28611159324646 seconds with loss  70.20965576171875\n",
            "\tEpoch  2  --- completed in  19.050713062286377 seconds with loss  62.66482162475586\n",
            "\tEpoch  3  --- completed in  19.22448182106018 seconds with loss  54.79573440551758\n",
            "\tEpoch  4  --- completed in  19.221070051193237 seconds with loss  46.965576171875\n",
            "\n",
            "Evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -503.3683776855469 %\n",
            "Graph  1 --- relative improvement percentage  -418.28036308288574 %\n",
            "Graph  2 --- relative improvement percentage  -475.327205657959 %\n",
            "Graph  3 --- relative improvement percentage  -455.1689624786377 %\n",
            "Graph  4 --- relative improvement percentage  -368.5298442840576 %\n",
            "\n",
            "Total training completed in  511.8840334415436 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AHYYpY5R7s0"
      },
      "source": [
        "initial_start = time.time()\n",
        "with torch.enable_grad():\n",
        "  training_set.resetDispatcher()\n",
        "  training_set.reset_w_hat()\n",
        "\n",
        "  for graph in training_set.batchesIndices: \n",
        "    print(\"Graph \", graph)\n",
        "    losses = []\n",
        "    for epoch in range(hyperparams['n_epochs']):\n",
        "      print(\"\\tEpoch \", epoch, end=\" \")\n",
        "      start = time.time()\n",
        "      for batch in range(training_set.graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "        A, X, E, _, x, y = training_set.getNextBatch(graph, hyperparams['batch_size'])\n",
        "        out = model(A, X, E)\n",
        "        training_set.store_w_hat(graph, out, x, y)\n",
        "\n",
        "      loss = training_set.rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "      losses.append(loss.item())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      training_set.reset_w_hat(graph)\n",
        "      print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())\n",
        "    print()\n",
        "    _ = model.evaluate(validation_set, hyperparams, verbose = True)\n",
        "    print()\n",
        "print(\"Total training completed in \", time.time()-initial_start, \"seconds\")\n",
        "print()\n",
        "_ = model.evaluate(test_set, hyperparams, verbose = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTvXUG56cXpH"
      },
      "source": [
        "# initial_start = time.time()\n",
        "\n",
        "# with torch.enable_grad():\n",
        "#   training_set.resetDispatcher()\n",
        "#   training_set.reset_w_hat()\n",
        "#   for e in range(hyperparams['n_epochs']):\n",
        "#     print(\"Big epoch \", e)\n",
        "#     for graph in training_set.batchesIndices: \n",
        "#       print(\"\\tGraph \", graph)\n",
        "#       losses = []\n",
        "#       for epoch in range(hyperparams['n_epochs'], ):\n",
        "#         print(\"\\t\\tEpoch \", epoch, end=\" \")\n",
        "#         start = time.time()\n",
        "#         for batch in range(training_set.graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "#           A, X, E, _, x, y = training_set.getNextBatch(graph, hyperparams['batch_size'])\n",
        "#           out = model(A, X, E)\n",
        "#           training_set.store_w_hat(graph, out, x, y)\n",
        "\n",
        "#         loss = training_set.rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "#         losses.append(loss.item())\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         optimizer.zero_grad()\n",
        "#         training_set.reset_w_hat(graph)\n",
        "#         print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())\n",
        "#     training_set.shuffle()\n",
        "# print(\"Total training completed in \", time.time()-initial_start, \"seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYZ_6OKsA_G9"
      },
      "source": [
        "# torch.save(model.state_dict(), \"state_dict_model2.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbCZeSRr9U-l"
      },
      "source": [
        "# model = GNN(hyperparams['embedding_dim'], hyperparams['n_layers']).float()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = hyperparams['lr'])\n",
        "\n",
        "# with torch.enable_grad():\n",
        "#   training_set.resetDispatcher()\n",
        "#   training_set.reset_w_hat()\n",
        "#   for epoch in range(hyperparams['n_epochs']):\n",
        "#     print(\"Epoch \", epoch)\n",
        "#     losses = []\n",
        "#     for graph in training_set.batchesIndices: \n",
        "#       print(\"\\tGraph \", graph, end=\" \")\n",
        "#       start = time.time()\n",
        "#       for batch in range(training_set.graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "#         A, X, E, _, x, y = training_set.getNextBatch(graph, hyperparams['batch_size'])\n",
        "#         out = model(A, X, E)\n",
        "#         training_set.store_w_hat(graph, out, x, y)\n",
        "\n",
        "#       loss = training_set.rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "#       losses.append(loss.item())\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#       optimizer.zero_grad()\n",
        "#       training_set.reset_w_hat(graph)\n",
        "#       print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())\n",
        "#     training_set.shuffle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZJgnr9sMIY_",
        "outputId": "933d2416-d5ee-466e-96ec-5c09e9a09c17"
      },
      "source": [
        "start = time.time()\n",
        "folder = '/content/drive/MyDrive/USI/GDLProject/'\n",
        "reduction_ratio = 0.5\n",
        "rr= str(reduction_ratio)\n",
        "training_name = folder+'training_set_'+rr+'.pbz2'\n",
        "valid_name = folder+'validation_set_'+rr+'.pbz2'\n",
        "test_name = folder+'test_set_'+rr+'.pbz2'\n",
        "\n",
        "\n",
        "# TRAINING SET\n",
        "start = time.time()\n",
        "print(\"Creating the training set...\")\n",
        "training_set = syntheticGraphDataset(entireMatrix=False)\n",
        "sizes = [i for i in range(512, 1012, 100)]\n",
        "preprocess = [True] * len(sizes)\n",
        "for key, s in enumerate(sizes):\n",
        "  print(\"\\t Creating graph \", key, \" of size \", s)\n",
        "  graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s,  p = (0.1*512)/s)\n",
        "  red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes=int(s*reduction_ratio))\n",
        "  training_set.addGraph(graph, red_graph, hyperparams['n_eig'], preprocessData = preprocess[key])\n",
        "print(\"Training set created in \", time.time()-start, \"seconds\\n\")\n",
        "\n",
        "syntheticGraphDataset.export_dataset(training_name,training_set)\n",
        "\n",
        "\n",
        "#VALIDATION SET\n",
        "start = time.time()\n",
        "print(\"Creating the validation set...\")\n",
        "validation_set = syntheticGraphDataset(entireMatrix=False)\n",
        "sizes = [i for i in range(1012, 1512, 100)]\n",
        "preprocess = [True]*len(sizes)\n",
        "for key, s in enumerate(sizes):\n",
        "  print(\"\\tCreating graph \", key, \" of size \", s)\n",
        "  graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s,  p = (0.1*512)/s)\n",
        "  red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes=int(s*reduction_ratio))\n",
        "  validation_set.addGraph(graph, red_graph, hyperparams['n_eig'], preprocessData = preprocess[key])\n",
        "print(\"Validation set created in \", time.time()-start, \"seconds\\n\")\n",
        "\n",
        "syntheticGraphDataset.export_dataset(valid_name,validation_set)\n",
        "\n",
        "\n",
        "# #TEST SET\n",
        "# start = time.time()\n",
        "\n",
        "# print(\"Creating the test set...\")\n",
        "# test_set = syntheticGraphDataset(entireMatrix=False)\n",
        "# sizes = [i for i in range(1512, 2913, 100)]\n",
        "# preprocess = [True]*len(sizes)\n",
        "# for key, s in enumerate(sizes):\n",
        "#   print(\"\\tCreating graph \", key, \" of size \",s)\n",
        "#   graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s, p = (0.1*512)/s)\n",
        "#   red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes = int(s*reduction_ratio))\n",
        "#   test_set.addGraph(graph, red_graph, hyperparams['n_eig'] , preprocessData = preprocess[key])\n",
        "# print(\"Dataset created in \", time.time() - start, \"seconds\")\n",
        "\n",
        "# syntheticGraphDataset.export_dataset(test_name, test_set)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the training set...\n",
            "\t Creating graph  0  of size  512\n",
            "\t Creating graph  1  of size  612\n",
            "\t Creating graph  2  of size  712\n",
            "\t Creating graph  3  of size  812\n",
            "\t Creating graph  4  of size  912\n",
            "Training set created in  17.795174837112427 seconds\n",
            "\n",
            "Creating the validation set...\n",
            "\tCreating graph  0  of size  1012\n",
            "\tCreating graph  1  of size  1112\n",
            "\tCreating graph  2  of size  1212\n",
            "\tCreating graph  3  of size  1312\n",
            "\tCreating graph  4  of size  1412\n",
            "Validation set created in  48.68718719482422 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go9R1etB3jZ5"
      },
      "source": [
        "start = time.time()\n",
        "folder = '/content/drive/MyDrive/USI/GDLProject/'\n",
        "reduction_ratio = 0.5\n",
        "rr= str(reduction_ratio)\n",
        "training_name = folder+'training_set_'+rr+'.pbz2'\n",
        "valid_name = folder+'validation_set_'+rr+'.pbz2'\n",
        "test_name = folder+'test_set_'+rr+'.pbz2'\n",
        "\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "print(\"Creating the training set...\")\n",
        "training_set = syntheticGraphDataset(entireMatrix=False)\n",
        "sizes = [i for i in range(512, 1012, 100)]\n",
        "preprocess = [True] * len(sizes)\n",
        "for key, s in enumerate(sizes):\n",
        "  print(\"\\t Creating graph \", key, \" of size \", s)\n",
        "  graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s,  p = (0.1*512)/s)\n",
        "  red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes=int(s*reduction_ratio))\n",
        "  training_set.addGraph(graph, red_graph, hyperparams['n_eig'], preprocessData = preprocess[key])\n",
        "print(\"Training set created in \", time.time()-start, \"seconds\\n\")\n",
        "\n",
        "syntheticGraphDataset.export_dataset(training_name,training_set)\n",
        "\n",
        "\n",
        "#VALIDATION SET\n",
        "start = time.time()\n",
        "print(\"Creating the validation set...\")\n",
        "validation_set = syntheticGraphDataset(entireMatrix=False)\n",
        "sizes = [i for i in range(1012, 1512, 100)]\n",
        "preprocess = [True]*len(sizes)\n",
        "for key, s in enumerate(sizes):\n",
        "  print(\"\\tCreating graph \", key, \" of size \", s)\n",
        "  graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s,  p = (0.1*512)/s)\n",
        "  red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes=int(s*reduction_ratio))\n",
        "  validation_set.addGraph(graph, red_graph, hyperparams['n_eig'], preprocessData = preprocess[key])\n",
        "print(\"Validation set created in \", time.time()-start, \"seconds\\n\")\n",
        "\n",
        "syntheticGraphDataset.export_dataset(valid_name,validation_set)\n",
        "\n",
        "\n",
        "#TEST SET\n",
        "start = time.time()\n",
        "\n",
        "print(\"Creating the test set...\")\n",
        "test_set = syntheticGraphDataset(entireMatrix=False)\n",
        "sizes = [i for i in range(1512, 2913, 100)]\n",
        "preprocess = [True]*len(sizes)\n",
        "for key, s in enumerate(sizes):\n",
        "  print(\"\\tCreating graph \", key, \" of size \",s)\n",
        "  graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s, p = (0.1*512)/s)\n",
        "  red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes = int(s*reduction_ratio))\n",
        "  test_set.addGraph(graph, red_graph, hyperparams['n_eig'] , preprocessData = preprocess[key])\n",
        "print(\"Dataset created in \", time.time() - start, \"seconds\")\n",
        "\n",
        "syntheticGraphDataset.export_dataset(test_name, test_set)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azfdRlO3tIJm"
      },
      "source": [
        "start = time.time()\n",
        "folder = '/content/drive/MyDrive/USI/GDLProject/'\n",
        "reduction_ratio = 0.7\n",
        "rr= str(reduction_ratio)\n",
        "training_name = folder+'training_set_'+rr+'.pbz2'\n",
        "valid_name = folder+'validation_set_'+rr+'.pbz2'\n",
        "test_name = folder+'test_set_'+rr+'.pbz2'\n",
        "\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "print(\"Creating the training set...\")\n",
        "training_set = syntheticGraphDataset(entireMatrix=False)\n",
        "sizes = [i for i in range(512, 1012, 100)]\n",
        "preprocess = [True] * len(sizes)\n",
        "for key, s in enumerate(sizes):\n",
        "  print(\"\\t Creating graph \", key, \" of size \", s)\n",
        "  graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s,  p = (0.1*512)/s)\n",
        "  red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes=int(s*reduction_ratio))\n",
        "  training_set.addGraph(graph, red_graph, hyperparams['n_eig'], preprocessData = preprocess[key])\n",
        "print(\"Training set created in \", time.time()-start, \"seconds\\n\")\n",
        "\n",
        "syntheticGraphDataset.export_dataset(training_name,training_set)\n",
        "\n",
        "\n",
        "#VALIDATION SET\n",
        "start = time.time()\n",
        "print(\"Creating the validation set...\")\n",
        "validation_set = syntheticGraphDataset(entireMatrix=False)\n",
        "sizes = [i for i in range(1012, 1512, 100)]\n",
        "preprocess = [True]*len(sizes)\n",
        "for key, s in enumerate(sizes):\n",
        "  print(\"\\tCreating graph \", key, \" of size \", s)\n",
        "  graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s,  p = (0.1*512)/s)\n",
        "  red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes=int(s*reduction_ratio))\n",
        "  validation_set.addGraph(graph, red_graph, hyperparams['n_eig'], preprocessData = preprocess[key])\n",
        "print(\"Validation set created in \", time.time()-start, \"seconds\\n\")\n",
        "\n",
        "syntheticGraphDataset.export_dataset(valid_name,validation_set)\n",
        "\n",
        "\n",
        "#TEST SET\n",
        "start = time.time()\n",
        "\n",
        "print(\"Creating the test set...\")\n",
        "test_set = syntheticGraphDataset(entireMatrix=False)\n",
        "sizes = [i for i in range(1512, 2913, 100)]\n",
        "preprocess = [True]*len(sizes)\n",
        "for key, s in enumerate(sizes):\n",
        "  print(\"\\tCreating graph \", key, \" of size \",s)\n",
        "  graph = syntheticGraph(name_graph_class='erdos_renyi_graph', size = s, p = (0.1*512)/s)\n",
        "  red_graph = reducedGraph(graph, coarse_type='baseline', n_supernodes = int(s*reduction_ratio))\n",
        "  test_set.addGraph(graph, red_graph, hyperparams['n_eig'] , preprocessData = preprocess[key])\n",
        "print(\"Dataset created in \", time.time() - start, \"seconds\")\n",
        "\n",
        "syntheticGraphDataset.export_dataset(test_name, test_set)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}