{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GDL_PROJECT_V_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVnGFCpxV09_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ecdb89-7637-4b67-9ea4-b0952001f62b"
      },
      "source": [
        "import torch\n",
        "try:\n",
        "  import torch_geometric\n",
        "except:\n",
        "  !pip -q install torch-scatter     -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-sparse      -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-cluster     -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "  !pip -q install torch-geometric\n",
        "  import torch_geometric\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.data import Data\n",
        "from scipy.sparse.csgraph import shortest_path\n",
        "from scipy.sparse import csr_matrix\n",
        "from torch_geometric.utils.random import erdos_renyi_graph\n",
        "import time\n",
        "import random\n",
        "from math import floor, ceil\n",
        "from copy import deepcopy\n",
        "import importlib\n",
        "from os import path\n",
        "import bz2\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "import sys\n",
        "\n",
        "from google.colab import drive\n",
        "folder = '/content/drive/MyDrive/USI/GDLProject/'\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        " \n",
        "sys.path.append(folder)\n",
        " \n",
        "\n",
        "import syntheticGraph\n",
        "import syntheticGraphDataset\n",
        "import reducedGraph\n",
        "import GINConv\n",
        "import GNN\n",
        "import graph\n",
        "\n",
        "importlib.reload(syntheticGraph)\n",
        "importlib.reload(syntheticGraphDataset)\n",
        "importlib.reload(reducedGraph)\n",
        "importlib.reload(GINConv)\n",
        "importlib.reload(GNN)\n",
        "importlib.reload(graph)\n",
        "\n",
        "from graph import Graph, Node\n",
        "from syntheticGraph import syntheticGraph\n",
        "from syntheticGraphDataset import syntheticGraphDataset\n",
        "from reducedGraph import reducedGraph\n",
        "from GINConv import GINConv\n",
        "from GNN import GNN\n",
        " \n",
        "torch.set_default_tensor_type(torch.FloatTensor)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSb4AfCGILHY"
      },
      "source": [
        "def train(model, datasets, hyperparams, verbose=False, exportName=None):\n",
        "  if (verbose): initial_start = time.time()\n",
        "  with torch.enable_grad():\n",
        "    datasets[\"training_set\"].resetDispatcher()\n",
        "    datasets[\"training_set\"].reset_w_hat()\n",
        "  \n",
        "    for graph in datasets[\"training_set\"].batchesIndices: \n",
        "      if (verbose): print(\"GRAPH \", graph)\n",
        "      losses = []\n",
        "      for epoch in range(hyperparams['n_epochs']):\n",
        "        if (verbose): print(\"\\tEpoch \", epoch, end=\" \")\n",
        "        start = time.time()\n",
        "        for batch in range(datasets[\"training_set\"].graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "          A, X, E, _, x, y = datasets[\"training_set\"].getNextBatch(graph, hyperparams['batch_size'])\n",
        "          out = model(A, X, E)\n",
        "          datasets[\"training_set\"].store_w_hat(graph, out, x, y)\n",
        "  \n",
        "        loss = datasets[\"training_set\"].rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "        losses.append(loss.item())\n",
        "        model.backpropagate(loss)\n",
        "        datasets[\"training_set\"].reset_w_hat(graph)\n",
        "        if (verbose): print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())\n",
        "      _ = model.evaluateRayleighLoss(datasets[\"validation_set\"], hyperparams, verbose = verbose)\n",
        "  if (verbose): print(\"Total training completed in \", time.time()-initial_start, \"seconds\")\n",
        "  if (exportName != None): model.export(exportName)\n",
        "  _ = model.evaluateRayleighLoss(datasets[\"test_set\"], hyperparams, verbose = verbose)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rDyWPbEQI6q"
      },
      "source": [
        "folder = '/content/drive/MyDrive/USI/GDLProject/'\n",
        "name_list = [\"training_set\", \"validation_set\", \"test_set\"]\n",
        "# reduction_ratio_list = [0.3, 0.5, 0.7]\n",
        "# name_graph_class_list = ['barabasi_albert_graph' ,'erdos_renyi_graph']\n",
        "# reducution_type_list = ['hem','baseline']\n",
        "reduction_ratio_list = [0.3]\n",
        "name_graph_class_list = ['erdos_renyi_graph']\n",
        "reducution_type_list = ['baseline']\n",
        "\n",
        "\n",
        "hyperparams = {\n",
        "    'loss_epoch' : 0,\n",
        "    'lr' : 0.001,\n",
        "    'embedding_dim' : 50,\n",
        "    'n_layers' : 3,\n",
        "    'n_eig' : 40,\n",
        "    'n_epochs' : 5,\n",
        "    'batch_size': 1,\n",
        "}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4eujxduHGKe",
        "outputId": "715e65ee-d6f7-491a-f6d7-c61014d827ad"
      },
      "source": [
        "datasets = {}\n",
        "\n",
        "for reduction_ratio in reduction_ratio_list:\n",
        "  for name_graph_class in name_graph_class_list:\n",
        "    for reducution_type in reducution_type_list:\n",
        "      \n",
        "      for name in name_list:\n",
        "        dataset_name = folder+name+'_'+str(reduction_ratio)+\"_\"+name_graph_class+'_'+reducution_type+'_'+'.pbz2'\n",
        "        datasets[name] = syntheticGraphDataset.import_dataset(dataset_name, verbose = True)\n",
        "\n",
        "      model = GNN(hyperparams['embedding_dim'], hyperparams['n_layers'], hyperparams['lr'], pathname = None)\n",
        "      train(model, datasets, hyperparams, verbose=True, exportName=None)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the compressed set...\n",
            "Dataset loaded in  6.855574607849121 seconds\n",
            "\n",
            "Loading the compressed set...\n",
            "Dataset loaded in  15.802812576293945 seconds\n",
            "\n",
            "Loading the compressed set...\n",
            "Dataset loaded in  111.97463059425354 seconds\n",
            "\n",
            "Graph  0\n",
            "\tEpoch  0  --- completed in  3.7288782596588135 seconds with loss  391.4488220214844\n",
            "\tEpoch  1  --- completed in  3.8231968879699707 seconds with loss  188.1591339111328\n",
            "\tEpoch  2  --- completed in  3.787081480026245 seconds with loss  57.38260269165039\n",
            "\tEpoch  3  --- completed in  3.7859508991241455 seconds with loss  12.497098922729492\n",
            "\tEpoch  4  --- completed in  3.855764150619507 seconds with loss  15.375471115112305\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -29.176482558250427 %\n",
            "Graph  1 --- relative improvement percentage  -32.583850622177124 %\n",
            "Graph  2 --- relative improvement percentage  1.4856829307973385 %\n",
            "Graph  3 --- relative improvement percentage  4.051648825407028 %\n",
            "Graph  4 --- relative improvement percentage  12.9293292760849 %\n",
            "Graph  1\n",
            "\tEpoch  0  --- completed in  5.526695489883423 seconds with loss  13.656222343444824\n",
            "\tEpoch  1  --- completed in  5.536709785461426 seconds with loss  13.850317001342773\n",
            "\tEpoch  2  --- completed in  5.506424427032471 seconds with loss  13.98057746887207\n",
            "\tEpoch  3  --- completed in  5.47308087348938 seconds with loss  14.096786499023438\n",
            "\tEpoch  4  --- completed in  5.591735601425171 seconds with loss  14.17736530303955\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -37.747031450271606 %\n",
            "Graph  1 --- relative improvement percentage  -42.66218841075897 %\n",
            "Graph  2 --- relative improvement percentage  -7.74606466293335 %\n",
            "Graph  3 --- relative improvement percentage  -4.184644669294357 %\n",
            "Graph  4 --- relative improvement percentage  4.501298442482948 %\n",
            "Graph  2\n",
            "\tEpoch  0  --- completed in  6.860816478729248 seconds with loss  12.897722244262695\n",
            "\tEpoch  1  --- completed in  6.8320019245147705 seconds with loss  12.897871971130371\n",
            "\tEpoch  2  --- completed in  6.709925889968872 seconds with loss  12.857250213623047\n",
            "\tEpoch  3  --- completed in  6.530646800994873 seconds with loss  12.775132179260254\n",
            "\tEpoch  4  --- completed in  6.673583745956421 seconds with loss  12.647181510925293\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -31.908151507377625 %\n",
            "Graph  1 --- relative improvement percentage  -35.36429703235626 %\n",
            "Graph  2 --- relative improvement percentage  -1.3079228810966015 %\n",
            "Graph  3 --- relative improvement percentage  0.7932812906801701 %\n",
            "Graph  4 --- relative improvement percentage  10.167238861322403 %\n",
            "Graph  3\n",
            "\tEpoch  0  --- completed in  8.371102809906006 seconds with loss  11.148177146911621\n",
            "\tEpoch  1  --- completed in  8.416014909744263 seconds with loss  10.967732429504395\n",
            "\tEpoch  2  --- completed in  8.353124618530273 seconds with loss  10.777950286865234\n",
            "\tEpoch  3  --- completed in  8.255001544952393 seconds with loss  10.572604179382324\n",
            "\tEpoch  4  --- completed in  8.2861168384552 seconds with loss  10.338957786560059\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  67.90075302124023 %\n",
            "Graph  1 --- relative improvement percentage  29.58919107913971 %\n",
            "Graph  2 --- relative improvement percentage  60.86580157279968 %\n",
            "Graph  3 --- relative improvement percentage  69.48447227478027 %\n",
            "Graph  4 --- relative improvement percentage  74.09742474555969 %\n",
            "Graph  4\n",
            "\tEpoch  0  --- completed in  10.591102838516235 seconds with loss  2.9143195152282715\n",
            "\tEpoch  1  --- completed in  10.715356588363647 seconds with loss  29.493268966674805\n",
            "\tEpoch  2  --- completed in  10.715926170349121 seconds with loss  10.646061897277832\n",
            "\tEpoch  3  --- completed in  10.633239030838013 seconds with loss  10.13097858428955\n",
            "\tEpoch  4  --- completed in  10.731733083724976 seconds with loss  10.687612533569336\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  -30.183011293411255 %\n",
            "Graph  1 --- relative improvement percentage  -33.08991193771362 %\n",
            "Graph  2 --- relative improvement percentage  0.33453144133090973 %\n",
            "Graph  3 --- relative improvement percentage  1.4971714466810226 %\n",
            "Graph  4 --- relative improvement percentage  11.36026531457901 %\n",
            "Total training completed in  293.9633164405823 seconds\n",
            "Rayleigh evaluation started..\n",
            "Graph  0 --- relative improvement percentage  5.518503114581108 %\n",
            "Graph  1 --- relative improvement percentage  13.450202345848083 %\n",
            "Graph  2 --- relative improvement percentage  33.20264220237732 %\n",
            "Graph  3 --- relative improvement percentage  31.501275300979614 %\n",
            "Graph  4 --- relative improvement percentage  37.226295471191406 %\n",
            "Graph  5 --- relative improvement percentage  34.228670597076416 %\n",
            "Graph  6 --- relative improvement percentage  54.26244139671326 %\n",
            "Graph  7 --- relative improvement percentage  40.63473343849182 %\n",
            "Graph  8 --- relative improvement percentage  38.56770098209381 %\n",
            "Graph  9 --- relative improvement percentage  50.38288235664368 %\n",
            "Graph  10 --- relative improvement percentage  39.47359025478363 %\n",
            "Graph  11 --- relative improvement percentage  60.166341066360474 %\n",
            "Graph  12 --- relative improvement percentage  60.76098680496216 %\n",
            "Graph  13 --- relative improvement percentage  62.19674348831177 %\n",
            "Graph  14 --- relative improvement percentage  57.340264320373535 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HpV9MamHFwP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTvXUG56cXpH"
      },
      "source": [
        "# initial_start = time.time()\n",
        "\n",
        "# with torch.enable_grad():\n",
        "#   training_set.resetDispatcher()\n",
        "#   training_set.reset_w_hat()\n",
        "#   for e in range(3):\n",
        "#     print(\"Big epoch \", e)\n",
        "#     for graph in training_set.batchesIndices: \n",
        "#       print(\"\\tGraph \", graph)\n",
        "#       losses = []\n",
        "#       for epoch in range(hyperparams['n_epochs'], ):\n",
        "#         print(\"\\t\\tEpoch \", epoch, end=\" \")\n",
        "#         start = time.time()\n",
        "#         for batch in range(training_set.graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "#           A, X, E, _, x, y = training_set.getNextBatch(graph, hyperparams['batch_size'])\n",
        "#           out = model(A, X, E)\n",
        "#           training_set.store_w_hat(graph, out, x, y)\n",
        "\n",
        "#         loss = training_set.rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "#         losses.append(loss.item())\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         optimizer.zero_grad()\n",
        "#         training_set.reset_w_hat(graph)\n",
        "#         print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())\n",
        "#     training_set.shuffle()\n",
        "# print(\"Total training completed in \", time.time()-initial_start, \"seconds\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbCZeSRr9U-l"
      },
      "source": [
        "# model = GNN(hyperparams['embedding_dim'], hyperparams['n_layers']).float()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = hyperparams['lr'])\n",
        "\n",
        "# with torch.enable_grad():\n",
        "#   training_set.resetDispatcher()\n",
        "#   training_set.reset_w_hat()\n",
        "#   for epoch in range(hyperparams['n_epochs']):\n",
        "#     print(\"Epoch \", epoch)\n",
        "#     losses = []\n",
        "#     for graph in training_set.batchesIndices: \n",
        "#       print(\"\\tGraph \", graph, end=\" \")\n",
        "#       start = time.time()\n",
        "#       for batch in range(training_set.graphNumberBatches(graph, hyperparams['batch_size'])):\n",
        "#         A, X, E, _, x, y = training_set.getNextBatch(graph, hyperparams['batch_size'])\n",
        "#         out = model(A, X, E)\n",
        "#         training_set.store_w_hat(graph, out, x, y)\n",
        "\n",
        "#       loss = training_set.rayleigh_loss(graph, hyperparams['n_eig'])\n",
        "#       losses.append(loss.item())\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#       optimizer.zero_grad()\n",
        "#       training_set.reset_w_hat(graph)\n",
        "#       print(\" --- completed in \", time.time()-start, \"seconds with loss \", loss.item())\n",
        "#     training_set.shuffle()"
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}